{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-16T15:28:23.001102Z",
     "start_time": "2024-01-16T15:28:19.932395Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Looking into tokenisation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f784dd279c496ffc"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "text = \"Tokenization is a fundamental preprocessing step in natural language processing (NLP). It involves breaking down a text into smaller units called tokens. Tokens are the building blocks of natural language and can be as short as a single character or as long as an entire word.\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T15:29:11.267336Z",
     "start_time": "2024-01-16T15:29:11.265993Z"
    }
   },
   "id": "ed8e51f15394d4ae"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T15:30:01.583312Z",
     "start_time": "2024-01-16T15:30:01.568251Z"
    }
   },
   "id": "54ebf30921e3fcb1"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "['Tokenization is a fundamental preprocessing step in natural language processing (NLP).',\n 'It involves breaking down a text into smaller units called tokens.',\n 'Tokens are the building blocks of natural language and can be as short as a single character or as long as an entire word.']"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T15:30:07.397495Z",
     "start_time": "2024-01-16T15:30:07.394734Z"
    }
   },
   "id": "8f317ff71aa9ae26"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T15:30:37.322045Z",
     "start_time": "2024-01-16T15:30:37.319415Z"
    }
   },
   "id": "585f61362f7a61c1"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "['Tokenization',\n 'is',\n 'a',\n 'fundamental',\n 'preprocessing',\n 'step',\n 'in',\n 'natural',\n 'language',\n 'processing',\n '(',\n 'NLP',\n ')',\n '.',\n 'It',\n 'involves',\n 'breaking',\n 'down',\n 'a',\n 'text',\n 'into',\n 'smaller',\n 'units',\n 'called',\n 'tokens',\n '.',\n 'Tokens',\n 'are',\n 'the',\n 'building',\n 'blocks',\n 'of',\n 'natural',\n 'language',\n 'and',\n 'can',\n 'be',\n 'as',\n 'short',\n 'as',\n 'a',\n 'single',\n 'character',\n 'or',\n 'as',\n 'long',\n 'as',\n 'an',\n 'entire',\n 'word',\n '.']"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T15:30:39.540932Z",
     "start_time": "2024-01-16T15:30:39.535224Z"
    }
   },
   "id": "d2d9fcbaad8e8f1c"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3   51\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences),\" \",len(words))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T15:31:06.346383Z",
     "start_time": "2024-01-16T15:31:06.342481Z"
    }
   },
   "id": "dd86d5237dfe9c0c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Looking into stemming and lemmatization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a81fff04dbec4ac"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T17:57:32.634849Z",
     "start_time": "2024-01-16T17:57:32.631432Z"
    }
   },
   "id": "7c5be339cd6be8a7"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "text_stem = \"Tokenization is a fundamental preprocessing step in natural language processing (NLP). It involves breaking down a text into smaller units called tokens. Tokens are the building blocks of natural language and can be as short as a single character or as long as an entire word. The process serves to structure and standardize textual data, facilitating tasks such as statistical analysis, language modeling, and information retrieval. Additionally, stemming is a related technique often employed in NLP preprocessing. Stemming aims to reduce words to their base or root form, aiding in the consolidation of similar words with shared meanings. This heuristic process involves removing prefixes or suffixes, resulting in stems that may not always be valid words but help in capturing the core meaning of related word forms. Stemming contributes to text normalization and is commonly used to enhance the efficiency of text analysis and information retrieval systems.\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T18:07:50.489682Z",
     "start_time": "2024-01-16T18:07:50.487873Z"
    }
   },
   "id": "4173199adea7ec42"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "sentences2 = nltk.sent_tokenize(text_stem)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T18:07:51.853210Z",
     "start_time": "2024-01-16T18:07:51.848541Z"
    }
   },
   "id": "edb7ea8e34f8f1b0"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization is a fundamental preprocessing step in natural language processing (NLP).', 'It involves breaking down a text into smaller units called tokens.', 'Tokens are the building blocks of natural language and can be as short as a single character or as long as an entire word.', 'The process serves to structure and standardize textual data, facilitating tasks such as statistical analysis, language modeling, and information retrieval.', 'Additionally, stemming is a related technique often employed in NLP preprocessing.', 'Stemming aims to reduce words to their base or root form, aiding in the consolidation of similar words with shared meanings.', 'This heuristic process involves removing prefixes or suffixes, resulting in stems that may not always be valid words but help in capturing the core meaning of related word forms.', 'Stemming contributes to text normalization and is commonly used to enhance the efficiency of text analysis and information retrieval systems.']\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "print(sentences2)\n",
    "print(len(sentences2))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T18:07:53.031162Z",
     "start_time": "2024-01-16T18:07:53.025949Z"
    }
   },
   "id": "efccc6e15dc9b2c1"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'a', 'fundamental', 'preprocessing', 'step', 'in', 'natural', 'language', 'processing', '(', 'NLP', ')', '.', 'It', 'involves', 'breaking', 'down', 'a', 'text', 'into', 'smaller', 'units', 'called', 'tokens', '.', 'Tokens', 'are', 'the', 'building', 'blocks', 'of', 'natural', 'language', 'and', 'can', 'be', 'as', 'short', 'as', 'a', 'single', 'character', 'or', 'as', 'long', 'as', 'an', 'entire', 'word', '.', 'The', 'process', 'serves', 'to', 'structure', 'and', 'standardize', 'textual', 'data', ',', 'facilitating', 'tasks', 'such', 'as', 'statistical', 'analysis', ',', 'language', 'modeling', ',', 'and', 'information', 'retrieval', '.', 'Additionally', ',', 'stemming', 'is', 'a', 'related', 'technique', 'often', 'employed', 'in', 'NLP', 'preprocessing', '.', 'Stemming', 'aims', 'to', 'reduce', 'words', 'to', 'their', 'base', 'or', 'root', 'form', ',', 'aiding', 'in', 'the', 'consolidation', 'of', 'similar', 'words', 'with', 'shared', 'meanings', '.', 'This', 'heuristic', 'process', 'involves', 'removing', 'prefixes', 'or', 'suffixes', ',', 'resulting', 'in', 'stems', 'that', 'may', 'not', 'always', 'be', 'valid', 'words', 'but', 'help', 'in', 'capturing', 'the', 'core', 'meaning', 'of', 'related', 'word', 'forms', '.', 'Stemming', 'contributes', 'to', 'text', 'normalization', 'and', 'is', 'commonly', 'used', 'to', 'enhance', 'the', 'efficiency', 'of', 'text', 'analysis', 'and', 'information', 'retrieval', 'systems', '.']\n",
      "163\n"
     ]
    }
   ],
   "source": [
    "words2 = nltk.word_tokenize(text_stem)\n",
    "print(words2)\n",
    "print(len(words2))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T18:07:54.069812Z",
     "start_time": "2024-01-16T18:07:54.066986Z"
    }
   },
   "id": "5884b31ab86f08f3"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T18:07:54.790655Z",
     "start_time": "2024-01-16T18:07:54.784957Z"
    }
   },
   "id": "e7d6d5a0046dbf5c"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "['i',\n 'me',\n 'my',\n 'myself',\n 'we',\n 'our',\n 'ours',\n 'ourselves',\n 'you',\n \"you're\",\n \"you've\",\n \"you'll\",\n \"you'd\",\n 'your',\n 'yours',\n 'yourself',\n 'yourselves',\n 'he',\n 'him',\n 'his',\n 'himself',\n 'she',\n \"she's\",\n 'her',\n 'hers',\n 'herself',\n 'it',\n \"it's\",\n 'its',\n 'itself',\n 'they',\n 'them',\n 'their',\n 'theirs',\n 'themselves',\n 'what',\n 'which',\n 'who',\n 'whom',\n 'this',\n 'that',\n \"that'll\",\n 'these',\n 'those',\n 'am',\n 'is',\n 'are',\n 'was',\n 'were',\n 'be',\n 'been',\n 'being',\n 'have',\n 'has',\n 'had',\n 'having',\n 'do',\n 'does',\n 'did',\n 'doing',\n 'a',\n 'an',\n 'the',\n 'and',\n 'but',\n 'if',\n 'or',\n 'because',\n 'as',\n 'until',\n 'while',\n 'of',\n 'at',\n 'by',\n 'for',\n 'with',\n 'about',\n 'against',\n 'between',\n 'into',\n 'through',\n 'during',\n 'before',\n 'after',\n 'above',\n 'below',\n 'to',\n 'from',\n 'up',\n 'down',\n 'in',\n 'out',\n 'on',\n 'off',\n 'over',\n 'under',\n 'again',\n 'further',\n 'then',\n 'once',\n 'here',\n 'there',\n 'when',\n 'where',\n 'why',\n 'how',\n 'all',\n 'any',\n 'both',\n 'each',\n 'few',\n 'more',\n 'most',\n 'other',\n 'some',\n 'such',\n 'no',\n 'nor',\n 'not',\n 'only',\n 'own',\n 'same',\n 'so',\n 'than',\n 'too',\n 'very',\n 's',\n 't',\n 'can',\n 'will',\n 'just',\n 'don',\n \"don't\",\n 'should',\n \"should've\",\n 'now',\n 'd',\n 'll',\n 'm',\n 'o',\n 're',\n 've',\n 'y',\n 'ain',\n 'aren',\n \"aren't\",\n 'couldn',\n \"couldn't\",\n 'didn',\n \"didn't\",\n 'doesn',\n \"doesn't\",\n 'hadn',\n \"hadn't\",\n 'hasn',\n \"hasn't\",\n 'haven',\n \"haven't\",\n 'isn',\n \"isn't\",\n 'ma',\n 'mightn',\n \"mightn't\",\n 'mustn',\n \"mustn't\",\n 'needn',\n \"needn't\",\n 'shan',\n \"shan't\",\n 'shouldn',\n \"shouldn't\",\n 'wasn',\n \"wasn't\",\n 'weren',\n \"weren't\",\n 'won',\n \"won't\",\n 'wouldn',\n \"wouldn't\"]"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english') # there are a total of 179 stop words i.e., words which are generally not too important and are meant to be filtered out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T18:07:55.343075Z",
     "start_time": "2024-01-16T18:07:55.334787Z"
    }
   },
   "id": "107dc4b2e1aa7315"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "stem_words_final = []\n",
    "for i in range(len(sentences2)):\n",
    "    words = nltk.word_tokenize(sentences2[i])\n",
    "    stem_words = [stemmer.stem(items) for items in words if items not in set(stopwords.words('english'))]\n",
    "    stem_words_final.append(stem_words)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T18:07:55.847371Z",
     "start_time": "2024-01-16T18:07:55.834089Z"
    }
   },
   "id": "c452f377c51927d4"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "[['token',\n  'fundament',\n  'preprocess',\n  'step',\n  'natur',\n  'languag',\n  'process',\n  '(',\n  'nlp',\n  ')',\n  '.'],\n ['it', 'involv', 'break', 'text', 'smaller', 'unit', 'call', 'token', '.'],\n ['token',\n  'build',\n  'block',\n  'natur',\n  'languag',\n  'short',\n  'singl',\n  'charact',\n  'long',\n  'entir',\n  'word',\n  '.'],\n ['the',\n  'process',\n  'serv',\n  'structur',\n  'standard',\n  'textual',\n  'data',\n  ',',\n  'facilit',\n  'task',\n  'statist',\n  'analysi',\n  ',',\n  'languag',\n  'model',\n  ',',\n  'inform',\n  'retriev',\n  '.'],\n ['addit',\n  ',',\n  'stem',\n  'relat',\n  'techniqu',\n  'often',\n  'employ',\n  'nlp',\n  'preprocess',\n  '.'],\n ['stem',\n  'aim',\n  'reduc',\n  'word',\n  'base',\n  'root',\n  'form',\n  ',',\n  'aid',\n  'consolid',\n  'similar',\n  'word',\n  'share',\n  'mean',\n  '.'],\n ['thi',\n  'heurist',\n  'process',\n  'involv',\n  'remov',\n  'prefix',\n  'suffix',\n  ',',\n  'result',\n  'stem',\n  'may',\n  'alway',\n  'valid',\n  'word',\n  'help',\n  'captur',\n  'core',\n  'mean',\n  'relat',\n  'word',\n  'form',\n  '.'],\n ['stem',\n  'contribut',\n  'text',\n  'normal',\n  'commonli',\n  'use',\n  'enhanc',\n  'effici',\n  'text',\n  'analysi',\n  'inform',\n  'retriev',\n  'system',\n  '.']]"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_words_final"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T18:07:56.373518Z",
     "start_time": "2024-01-16T18:07:56.370575Z"
    }
   },
   "id": "bdb15f04b6a1d76"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T18:07:58.019551Z",
     "start_time": "2024-01-16T18:07:58.014637Z"
    }
   },
   "id": "ba77501e798a1352"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "lemma_words_final = []\n",
    "for i in range(len(sentences2)):\n",
    "    words = nltk.word_tokenize(sentences2[i])\n",
    "    lemma_words = [lemmatizer.lemmatize(items) for items in words if items not in set(stopwords.words('english'))]\n",
    "    lemma_words_final.append(lemma_words)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T18:07:58.689573Z",
     "start_time": "2024-01-16T18:07:58.668720Z"
    }
   },
   "id": "a0086e25bb35ff58"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "[['Tokenization',\n  'fundamental',\n  'preprocessing',\n  'step',\n  'natural',\n  'language',\n  'processing',\n  '(',\n  'NLP',\n  ')',\n  '.'],\n ['It',\n  'involves',\n  'breaking',\n  'text',\n  'smaller',\n  'unit',\n  'called',\n  'token',\n  '.'],\n ['Tokens',\n  'building',\n  'block',\n  'natural',\n  'language',\n  'short',\n  'single',\n  'character',\n  'long',\n  'entire',\n  'word',\n  '.'],\n ['The',\n  'process',\n  'serf',\n  'structure',\n  'standardize',\n  'textual',\n  'data',\n  ',',\n  'facilitating',\n  'task',\n  'statistical',\n  'analysis',\n  ',',\n  'language',\n  'modeling',\n  ',',\n  'information',\n  'retrieval',\n  '.'],\n ['Additionally',\n  ',',\n  'stemming',\n  'related',\n  'technique',\n  'often',\n  'employed',\n  'NLP',\n  'preprocessing',\n  '.'],\n ['Stemming',\n  'aim',\n  'reduce',\n  'word',\n  'base',\n  'root',\n  'form',\n  ',',\n  'aiding',\n  'consolidation',\n  'similar',\n  'word',\n  'shared',\n  'meaning',\n  '.'],\n ['This',\n  'heuristic',\n  'process',\n  'involves',\n  'removing',\n  'prefix',\n  'suffix',\n  ',',\n  'resulting',\n  'stem',\n  'may',\n  'always',\n  'valid',\n  'word',\n  'help',\n  'capturing',\n  'core',\n  'meaning',\n  'related',\n  'word',\n  'form',\n  '.'],\n ['Stemming',\n  'contributes',\n  'text',\n  'normalization',\n  'commonly',\n  'used',\n  'enhance',\n  'efficiency',\n  'text',\n  'analysis',\n  'information',\n  'retrieval',\n  'system',\n  '.']]"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_words_final"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T18:07:59.404952Z",
     "start_time": "2024-01-16T18:07:59.400943Z"
    }
   },
   "id": "689fffca725756a4"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "['Tokenization is a fundamental preprocessing step in natural language processing (NLP).',\n 'It involves breaking down a text into smaller units called tokens.',\n 'Tokens are the building blocks of natural language and can be as short as a single character or as long as an entire word.',\n 'The process serves to structure and standardize textual data, facilitating tasks such as statistical analysis, language modeling, and information retrieval.',\n 'Additionally, stemming is a related technique often employed in NLP preprocessing.',\n 'Stemming aims to reduce words to their base or root form, aiding in the consolidation of similar words with shared meanings.',\n 'This heuristic process involves removing prefixes or suffixes, resulting in stems that may not always be valid words but help in capturing the core meaning of related word forms.',\n 'Stemming contributes to text normalization and is commonly used to enhance the efficiency of text analysis and information retrieval systems.']"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T18:07:59.996663Z",
     "start_time": "2024-01-16T18:07:59.992866Z"
    }
   },
   "id": "9019b56e53bc5b7c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Looking into Bag of words"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ceec53a5d3d2225"
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T18:45:28.853400Z",
     "start_time": "2024-01-16T18:45:28.837162Z"
    }
   },
   "id": "c547e2a5da421c7"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "text_bow = \"Tokenization is a fundamental preprocessing step in natural language processing (NLP). It involves breaking down a text into smaller units called tokens. Tokens are the building blocks of natural language and can be as short as a single character or as long as an entire word. The process serves to structure and standardize textual data, facilitating tasks such as statistical analysis, language modeling, and information retrieval. Additionally, stemming is a related technique often employed in NLP preprocessing. Stemming aims to reduce words to their base or root form, aiding in the consolidation of similar words with shared meanings. This heuristic process involves removing prefixes or suffixes, resulting in stems that may not always be valid words but help in capturing the core meaning of related word forms. Stemming contributes to text normalization and is commonly used to enhance the efficiency of text analysis and information retrieval systems.In conjunction with tokenization and stemming, the Bag of Words (BoW) model is frequently employed in NLP. BoW represents a document as an unordered set of its constituent words, disregarding grammar and word order but capturing word frequency information. Each unique word in the document forms a token, and the resulting document is represented as a numerical vector, where each element corresponds to the frequency of a specific word in the vocabulary. BoW is a foundational technique in various NLP tasks, providing a simple yet effective way to represent and analyze textual data. While it lacks semantic understanding and context, BoW serves as a baseline for more advanced models and is particularly useful in applications such as text classification, sentiment analysis, and information retrieval.\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T18:29:37.226830Z",
     "start_time": "2024-01-16T18:29:37.222493Z"
    }
   },
   "id": "cd68ebce9afc0dc3"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "wordnet = WordNetLemmatizer()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T18:30:02.193965Z",
     "start_time": "2024-01-16T18:30:02.177769Z"
    }
   },
   "id": "882f775e84fa351"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "12"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences3 = nltk.sent_tokenize(text_bow)\n",
    "len(sentences3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T18:30:52.286179Z",
     "start_time": "2024-01-16T18:30:52.243042Z"
    }
   },
   "id": "b206beea98bb60fb"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "['Tokenization is a fundamental preprocessing step in natural language processing (NLP).',\n 'It involves breaking down a text into smaller units called tokens.',\n 'Tokens are the building blocks of natural language and can be as short as a single character or as long as an entire word.',\n 'The process serves to structure and standardize textual data, facilitating tasks such as statistical analysis, language modeling, and information retrieval.',\n 'Additionally, stemming is a related technique often employed in NLP preprocessing.',\n 'Stemming aims to reduce words to their base or root form, aiding in the consolidation of similar words with shared meanings.',\n 'This heuristic process involves removing prefixes or suffixes, resulting in stems that may not always be valid words but help in capturing the core meaning of related word forms.',\n 'Stemming contributes to text normalization and is commonly used to enhance the efficiency of text analysis and information retrieval systems.In conjunction with tokenization and stemming, the Bag of Words (BoW) model is frequently employed in NLP.',\n 'BoW represents a document as an unordered set of its constituent words, disregarding grammar and word order but capturing word frequency information.',\n 'Each unique word in the document forms a token, and the resulting document is represented as a numerical vector, where each element corresponds to the frequency of a specific word in the vocabulary.',\n 'BoW is a foundational technique in various NLP tasks, providing a simple yet effective way to represent and analyze textual data.',\n 'While it lacks semantic understanding and context, BoW serves as a baseline for more advanced models and is particularly useful in applications such as text classification, sentiment analysis, and information retrieval.']"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences3"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T18:31:10.648379Z",
     "start_time": "2024-01-16T18:31:10.637961Z"
    }
   },
   "id": "b6edf30c3df439a9"
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token fundament preprocess step natur languag process nlp\n",
      "involv break text smaller unit call token\n",
      "token build block natur languag short singl charact long entir word\n",
      "process serv structur standard textual data facilit task statist analysi languag model inform retriev\n",
      "addit stem relat techniqu often employ nlp preprocess\n",
      "stem aim reduc word base root form aid consolid similar word share mean\n",
      "heurist process involv remov prefix suffix result stem may alway valid word help captur core mean relat word form\n",
      "stem contribut text normal commonli use enhanc effici text analysi inform retriev system conjunct token stem bag word bow model frequent employ nlp\n",
      "bow repres document unord set constitu word disregard grammar word order captur word frequenc inform\n",
      "uniqu word document form token result document repres numer vector element correspond frequenc specif word vocabulari\n",
      "bow foundat techniqu variou nlp task provid simpl yet effect way repres analyz textual data\n",
      "lack semant understand context bow serv baselin advanc model particularli use applic text classif sentiment analysi inform retriev\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "for i in range(len(sentences3)):\n",
    "    review = re.sub(\"[^a-zA-Z]\",' ',sentences3[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [ps.stem(item) for item in review if item not in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "    print(review)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T18:44:30.509775Z",
     "start_time": "2024-01-16T18:44:30.489534Z"
    }
   },
   "id": "bd6cfff30fd10981"
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "data": {
      "text/plain": "['token fundament preprocess step natur languag process nlp',\n 'involv break text smaller unit call token',\n 'token build block natur languag short singl charact long entir word',\n 'process serv structur standard textual data facilit task statist analysi languag model inform retriev',\n 'addit stem relat techniqu often employ nlp preprocess',\n 'stem aim reduc word base root form aid consolid similar word share mean',\n 'heurist process involv remov prefix suffix result stem may alway valid word help captur core mean relat word form',\n 'stem contribut text normal commonli use enhanc effici text analysi inform retriev system conjunct token stem bag word bow model frequent employ nlp',\n 'bow repres document unord set constitu word disregard grammar word order captur word frequenc inform',\n 'uniqu word document form token result document repres numer vector element correspond frequenc specif word vocabulari',\n 'bow foundat techniqu variou nlp task provid simpl yet effect way repres analyz textual data',\n 'lack semant understand context bow serv baselin advanc model particularli use applic text classif sentiment analysi inform retriev']"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T18:44:31.698181Z",
     "start_time": "2024-01-16T18:44:31.688131Z"
    }
   },
   "id": "25571ba7ad4456a1"
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenization fundamental preprocessing step natural language processing nlp\n",
      "involves breaking text smaller unit called token\n",
      "token building block natural language short single character long entire word\n",
      "process serf structure standardize textual data facilitating task statistical analysis language modeling information retrieval\n",
      "additionally stemming related technique often employed nlp preprocessing\n",
      "stemming aim reduce word base root form aiding consolidation similar word shared meaning\n",
      "heuristic process involves removing prefix suffix resulting stem may always valid word help capturing core meaning related word form\n",
      "stemming contributes text normalization commonly used enhance efficiency text analysis information retrieval system conjunction tokenization stemming bag word bow model frequently employed nlp\n",
      "bow represents document unordered set constituent word disregarding grammar word order capturing word frequency information\n",
      "unique word document form token resulting document represented numerical vector element corresponds frequency specific word vocabulary\n",
      "bow foundational technique various nlp task providing simple yet effective way represent analyze textual data\n",
      "lack semantic understanding context bow serf baseline advanced model particularly useful application text classification sentiment analysis information retrieval\n"
     ]
    }
   ],
   "source": [
    "corpus_lemma = []\n",
    "for i in range(len(sentences3)):\n",
    "    review = re.sub(\"[^a-zA-Z]\",' ',sentences3[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [wordnet.lemmatize(item) for item in review if item not in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus_lemma.append(review)\n",
    "    print(review)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T18:44:25.555555Z",
     "start_time": "2024-01-16T18:44:25.530457Z"
    }
   },
   "id": "34fb6a9465a8df9e"
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tokenization fundamental preprocessing step natural language processing nlp', 'involves breaking text smaller unit called token', 'token building block natural language short single character long entire word', 'process serf structure standardize textual data facilitating task statistical analysis language modeling information retrieval', 'additionally stemming related technique often employed nlp preprocessing', 'stemming aim reduce word base root form aiding consolidation similar word shared meaning', 'heuristic process involves removing prefix suffix resulting stem may always valid word help capturing core meaning related word form', 'stemming contributes text normalization commonly used enhance efficiency text analysis information retrieval system conjunction tokenization stemming bag word bow model frequently employed nlp', 'bow represents document unordered set constituent word disregarding grammar word order capturing word frequency information', 'unique word document form token resulting document represented numerical vector element corresponds frequency specific word vocabulary', 'bow foundational technique various nlp task providing simple yet effective way represent analyze textual data', 'lack semantic understanding context bow serf baseline advanced model particularly useful application text classification sentiment analysis information retrieval']\n"
     ]
    }
   ],
   "source": [
    "print(corpus_lemma)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T18:44:26.976238Z",
     "start_time": "2024-01-16T18:44:26.967090Z"
    }
   },
   "id": "7488340f19ed4ae"
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "# creating the bag of words using Count Vectorizer\n",
    "cv = CountVectorizer()\n",
    "x = cv.fit_transform(corpus_lemma).toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T18:46:23.894355Z",
     "start_time": "2024-01-16T18:46:23.874675Z"
    }
   },
   "id": "4b3dbeab6bdb008b"
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 1, 0],\n       ...,\n       [0, 0, 0, ..., 0, 2, 0],\n       [0, 0, 0, ..., 1, 0, 1],\n       [0, 1, 0, ..., 0, 0, 0]])"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T18:46:33.024745Z",
     "start_time": "2024-01-16T18:46:32.998142Z"
    }
   },
   "id": "8f2bcebadadb30ad"
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['additionally', 'advanced', 'aiding', 'aim', 'always', 'analysis',\n       'analyze', 'application', 'bag', 'base', 'baseline', 'block',\n       'bow', 'breaking', 'building', 'called', 'capturing', 'character',\n       'classification', 'commonly', 'conjunction', 'consolidation',\n       'constituent', 'context', 'contributes', 'core', 'corresponds',\n       'data', 'disregarding', 'document', 'effective', 'efficiency',\n       'element', 'employed', 'enhance', 'entire', 'facilitating', 'form',\n       'foundational', 'frequency', 'frequently', 'fundamental',\n       'grammar', 'help', 'heuristic', 'information', 'involves', 'lack',\n       'language', 'long', 'may', 'meaning', 'model', 'modeling',\n       'natural', 'nlp', 'normalization', 'numerical', 'often', 'order',\n       'particularly', 'prefix', 'preprocessing', 'process', 'processing',\n       'providing', 'reduce', 'related', 'removing', 'represent',\n       'represented', 'represents', 'resulting', 'retrieval', 'root',\n       'semantic', 'sentiment', 'serf', 'set', 'shared', 'short',\n       'similar', 'simple', 'single', 'smaller', 'specific',\n       'standardize', 'statistical', 'stem', 'stemming', 'step',\n       'structure', 'suffix', 'system', 'task', 'technique', 'text',\n       'textual', 'token', 'tokenization', 'understanding', 'unique',\n       'unit', 'unordered', 'used', 'useful', 'valid', 'various',\n       'vector', 'vocabulary', 'way', 'word', 'yet'], dtype=object)"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names_out()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T18:46:45.799845Z",
     "start_time": "2024-01-16T18:46:45.783761Z"
    }
   },
   "id": "98e31c9562bb8aff"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Looking into TF-IDF"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a14e0fcaaaf53c76"
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T19:20:33.972754Z",
     "start_time": "2024-01-16T19:20:33.969092Z"
    }
   },
   "id": "809f4cfb07f47321"
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "text_tfidf = \"Tokenization is a fundamental preprocessing step in natural language processing (NLP). It involves breaking down a text into smaller units called tokens. Tokens are the building blocks of natural language and can be as short as a single character or as long as an entire word. The process serves to structure and standardize textual data, facilitating tasks such as statistical analysis, language modeling, and information retrieval. Additionally, stemming is a related technique often employed in NLP preprocessing. Stemming aims to reduce words to their base or root form, aiding in the consolidation of similar words with shared meanings. This heuristic process involves removing prefixes or suffixes, resulting in stems that may not always be valid words but help in capturing the core meaning of related word forms. Stemming contributes to text normalization and is commonly used to enhance the efficiency of text analysis and information retrieval systems. In conjunction with tokenization and stemming, the Bag of Words (BoW) model is frequently employed in NLP. BoW represents a document as an unordered set of its constituent words, disregarding grammar and word order but capturing word frequency information. Each unique word in the document forms a token, and the resulting document is represented as a numerical vector, where each element corresponds to the frequency of a specific word in the vocabulary. BoW is a foundational technique in various NLP tasks, providing a simple yet effective way to represent and analyze textual data. While it lacks semantic understanding and context, BoW serves as a baseline for more advanced models and is particularly useful in applications such as text classification, sentiment analysis, and information retrieval. Additionally, TF-IDF (Term Frequency-Inverse Document Frequency) is a numerical statistic used in NLP and information retrieval to evaluate the importance of a word in a document relative to a collection of documents (corpus). TF-IDF takes into account both the frequency of a term within a document (TF) and the rarity of the term across the entire corpus (IDF). It helps in weighting the importance of terms, emphasizing those that are frequent in a document but rare in the overall corpus. TF-IDF is commonly used for tasks such as document ranking, information retrieval, and text mining. It provides a more nuanced representation of documents compared to simple word frequencies, making it a valuable tool in natural language processing.\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T19:21:53.799181Z",
     "start_time": "2024-01-16T19:21:53.792862Z"
    }
   },
   "id": "f1eaf369aa796447"
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "data": {
      "text/plain": "18"
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk.sent_tokenize(text_tfidf))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T19:21:58.923577Z",
     "start_time": "2024-01-16T19:21:58.916284Z"
    }
   },
   "id": "bd4c0430734c2e39"
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "wordnet = WordNetLemmatizer() \n",
    "corpus= []"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T19:22:01.097636Z",
     "start_time": "2024-01-16T19:22:01.088306Z"
    }
   },
   "id": "141166a9bedc47cd"
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "sentences4 = nltk.sent_tokenize(text_tfidf)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T19:22:03.859891Z",
     "start_time": "2024-01-16T19:22:03.827348Z"
    }
   },
   "id": "42e0aef2e6169b60"
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "data": {
      "text/plain": "['Tokenization is a fundamental preprocessing step in natural language processing (NLP).',\n 'It involves breaking down a text into smaller units called tokens.',\n 'Tokens are the building blocks of natural language and can be as short as a single character or as long as an entire word.',\n 'The process serves to structure and standardize textual data, facilitating tasks such as statistical analysis, language modeling, and information retrieval.',\n 'Additionally, stemming is a related technique often employed in NLP preprocessing.',\n 'Stemming aims to reduce words to their base or root form, aiding in the consolidation of similar words with shared meanings.',\n 'This heuristic process involves removing prefixes or suffixes, resulting in stems that may not always be valid words but help in capturing the core meaning of related word forms.',\n 'Stemming contributes to text normalization and is commonly used to enhance the efficiency of text analysis and information retrieval systems.',\n 'In conjunction with tokenization and stemming, the Bag of Words (BoW) model is frequently employed in NLP.',\n 'BoW represents a document as an unordered set of its constituent words, disregarding grammar and word order but capturing word frequency information.',\n 'Each unique word in the document forms a token, and the resulting document is represented as a numerical vector, where each element corresponds to the frequency of a specific word in the vocabulary.',\n 'BoW is a foundational technique in various NLP tasks, providing a simple yet effective way to represent and analyze textual data.',\n 'While it lacks semantic understanding and context, BoW serves as a baseline for more advanced models and is particularly useful in applications such as text classification, sentiment analysis, and information retrieval.',\n 'Additionally, TF-IDF (Term Frequency-Inverse Document Frequency) is a numerical statistic used in NLP and information retrieval to evaluate the importance of a word in a document relative to a collection of documents (corpus).',\n 'TF-IDF takes into account both the frequency of a term within a document (TF) and the rarity of the term across the entire corpus (IDF).',\n 'It helps in weighting the importance of terms, emphasizing those that are frequent in a document but rare in the overall corpus.',\n 'TF-IDF is commonly used for tasks such as document ranking, information retrieval, and text mining.',\n 'It provides a more nuanced representation of documents compared to simple word frequencies, making it a valuable tool in natural language processing.']"
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences4"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T19:22:07.685102Z",
     "start_time": "2024-01-16T19:22:07.645664Z"
    }
   },
   "id": "7451c8343f1d09fd"
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenization fundamental preprocessing step natural language processing nlp\n",
      "involves breaking text smaller unit called token\n",
      "token building block natural language short single character long entire word\n",
      "process serf structure standardize textual data facilitating task statistical analysis language modeling information retrieval\n",
      "additionally stemming related technique often employed nlp preprocessing\n",
      "stemming aim reduce word base root form aiding consolidation similar word shared meaning\n",
      "heuristic process involves removing prefix suffix resulting stem may always valid word help capturing core meaning related word form\n",
      "stemming contributes text normalization commonly used enhance efficiency text analysis information retrieval system\n",
      "conjunction tokenization stemming bag word bow model frequently employed nlp\n",
      "bow represents document unordered set constituent word disregarding grammar word order capturing word frequency information\n",
      "unique word document form token resulting document represented numerical vector element corresponds frequency specific word vocabulary\n",
      "bow foundational technique various nlp task providing simple yet effective way represent analyze textual data\n",
      "lack semantic understanding context bow serf baseline advanced model particularly useful application text classification sentiment analysis information retrieval\n",
      "additionally tf idf term frequency inverse document frequency numerical statistic used nlp information retrieval evaluate importance word document relative collection document corpus\n",
      "tf idf take account frequency term within document tf rarity term across entire corpus idf\n",
      "help weighting importance term emphasizing frequent document rare overall corpus\n",
      "tf idf commonly used task document ranking information retrieval text mining\n",
      "provides nuanced representation document compared simple word frequency making valuable tool natural language processing\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sentences4)):\n",
    "    review = re.sub(\"[^a-zA-Z]\",' ',sentences4[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [wordnet.lemmatize(item) for item in review if item not in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "    print(review)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T19:27:47.829362Z",
     "start_time": "2024-01-16T19:27:47.796713Z"
    }
   },
   "id": "cfbfa78455a3fc54"
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tokenization fundamental preprocessing step natural language processing nlp', 'involves breaking text smaller unit called token', 'token building block natural language short single character long entire word', 'process serf structure standardize textual data facilitating task statistical analysis language modeling information retrieval', 'additionally stemming related technique often employed nlp preprocessing', 'stemming aim reduce word base root form aiding consolidation similar word shared meaning', 'heuristic process involves removing prefix suffix resulting stem may always valid word help capturing core meaning related word form', 'stemming contributes text normalization commonly used enhance efficiency text analysis information retrieval system', 'conjunction tokenization stemming bag word bow model frequently employed nlp', 'bow represents document unordered set constituent word disregarding grammar word order capturing word frequency information', 'unique word document form token resulting document represented numerical vector element corresponds frequency specific word vocabulary', 'bow foundational technique various nlp task providing simple yet effective way represent analyze textual data', 'lack semantic understanding context bow serf baseline advanced model particularly useful application text classification sentiment analysis information retrieval', 'additionally tf idf term frequency inverse document frequency numerical statistic used nlp information retrieval evaluate importance word document relative collection document corpus', 'tf idf take account frequency term within document tf rarity term across entire corpus idf', 'help weighting importance term emphasizing frequent document rare overall corpus', 'tf idf commonly used task document ranking information retrieval text mining', 'provides nuanced representation document compared simple word frequency making valuable tool natural language processing']\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T19:27:57.369506Z",
     "start_time": "2024-01-16T19:27:57.333303Z"
    }
   },
   "id": "d5f07416521097d9"
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "# creating the TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(corpus).toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T19:29:12.871565Z",
     "start_time": "2024-01-16T19:29:12.832838Z"
    }
   },
   "id": "3df6cae3b8cdc2aa"
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 0.        , 0.18106353,\n        0.        ],\n       ...,\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 0.        , 0.16410647,\n        0.        ]])"
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T19:29:14.188607Z",
     "start_time": "2024-01-16T19:29:14.161236Z"
    }
   },
   "id": "86bf6a248cb6677a"
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['account', 'across', 'additionally', 'advanced', 'aiding', 'aim',\n       'always', 'analysis', 'analyze', 'application', 'bag', 'base',\n       'baseline', 'block', 'bow', 'breaking', 'building', 'called',\n       'capturing', 'character', 'classification', 'collection',\n       'commonly', 'compared', 'conjunction', 'consolidation',\n       'constituent', 'context', 'contributes', 'core', 'corpus',\n       'corresponds', 'data', 'disregarding', 'document', 'effective',\n       'efficiency', 'element', 'emphasizing', 'employed', 'enhance',\n       'entire', 'evaluate', 'facilitating', 'form', 'foundational',\n       'frequency', 'frequent', 'frequently', 'fundamental', 'grammar',\n       'help', 'heuristic', 'idf', 'importance', 'information', 'inverse',\n       'involves', 'lack', 'language', 'long', 'making', 'may', 'meaning',\n       'mining', 'model', 'modeling', 'natural', 'nlp', 'normalization',\n       'nuanced', 'numerical', 'often', 'order', 'overall',\n       'particularly', 'prefix', 'preprocessing', 'process', 'processing',\n       'provides', 'providing', 'ranking', 'rare', 'rarity', 'reduce',\n       'related', 'relative', 'removing', 'represent', 'representation',\n       'represented', 'represents', 'resulting', 'retrieval', 'root',\n       'semantic', 'sentiment', 'serf', 'set', 'shared', 'short',\n       'similar', 'simple', 'single', 'smaller', 'specific',\n       'standardize', 'statistic', 'statistical', 'stem', 'stemming',\n       'step', 'structure', 'suffix', 'system', 'take', 'task',\n       'technique', 'term', 'text', 'textual', 'tf', 'token',\n       'tokenization', 'tool', 'understanding', 'unique', 'unit',\n       'unordered', 'used', 'useful', 'valid', 'valuable', 'various',\n       'vector', 'vocabulary', 'way', 'weighting', 'within', 'word',\n       'yet'], dtype=object)"
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.get_feature_names_out()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T19:29:32.760506Z",
     "start_time": "2024-01-16T19:29:32.727399Z"
    }
   },
   "id": "43586d21bca60362"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "53c694d450888b92"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
