{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-16T15:28:23.001102Z",
     "start_time": "2024-01-16T15:28:19.932395Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Looking into tokenisation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f784dd279c496ffc"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "text = \"Tokenization is a fundamental preprocessing step in natural language processing (NLP). It involves breaking down a text into smaller units called tokens. Tokens are the building blocks of natural language and can be as short as a single character or as long as an entire word.\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T15:29:11.267336Z",
     "start_time": "2024-01-16T15:29:11.265993Z"
    }
   },
   "id": "ed8e51f15394d4ae"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T15:30:01.583312Z",
     "start_time": "2024-01-16T15:30:01.568251Z"
    }
   },
   "id": "54ebf30921e3fcb1"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "['Tokenization is a fundamental preprocessing step in natural language processing (NLP).',\n 'It involves breaking down a text into smaller units called tokens.',\n 'Tokens are the building blocks of natural language and can be as short as a single character or as long as an entire word.']"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T15:30:07.397495Z",
     "start_time": "2024-01-16T15:30:07.394734Z"
    }
   },
   "id": "8f317ff71aa9ae26"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T15:30:37.322045Z",
     "start_time": "2024-01-16T15:30:37.319415Z"
    }
   },
   "id": "585f61362f7a61c1"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "['Tokenization',\n 'is',\n 'a',\n 'fundamental',\n 'preprocessing',\n 'step',\n 'in',\n 'natural',\n 'language',\n 'processing',\n '(',\n 'NLP',\n ')',\n '.',\n 'It',\n 'involves',\n 'breaking',\n 'down',\n 'a',\n 'text',\n 'into',\n 'smaller',\n 'units',\n 'called',\n 'tokens',\n '.',\n 'Tokens',\n 'are',\n 'the',\n 'building',\n 'blocks',\n 'of',\n 'natural',\n 'language',\n 'and',\n 'can',\n 'be',\n 'as',\n 'short',\n 'as',\n 'a',\n 'single',\n 'character',\n 'or',\n 'as',\n 'long',\n 'as',\n 'an',\n 'entire',\n 'word',\n '.']"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T15:30:39.540932Z",
     "start_time": "2024-01-16T15:30:39.535224Z"
    }
   },
   "id": "d2d9fcbaad8e8f1c"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3   51\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences),\" \",len(words))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T15:31:06.346383Z",
     "start_time": "2024-01-16T15:31:06.342481Z"
    }
   },
   "id": "dd86d5237dfe9c0c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Looking into stemming and lemmatization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a81fff04dbec4ac"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T17:57:32.634849Z",
     "start_time": "2024-01-16T17:57:32.631432Z"
    }
   },
   "id": "7c5be339cd6be8a7"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "text_stem = \"Tokenization is a fundamental preprocessing step in natural language processing (NLP). It involves breaking down a text into smaller units called tokens. Tokens are the building blocks of natural language and can be as short as a single character or as long as an entire word. The process serves to structure and standardize textual data, facilitating tasks such as statistical analysis, language modeling, and information retrieval.Additionally, stemming is a related technique often employed in NLP preprocessing. Stemming aims to reduce words to their base or root form, aiding in the consolidation of similar words with shared meanings. This heuristic process involves removing prefixes or suffixes, resulting in stems that may not always be valid words but help in capturing the core meaning of related word forms. Stemming contributes to text normalization and is commonly used to enhance the efficiency of text analysis and information retrieval systems.\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T15:50:24.813503Z",
     "start_time": "2024-01-16T15:50:24.812822Z"
    }
   },
   "id": "4173199adea7ec42"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "sentences2 = nltk.sent_tokenize(text_stem)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T15:55:44.145003Z",
     "start_time": "2024-01-16T15:55:44.140514Z"
    }
   },
   "id": "edb7ea8e34f8f1b0"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization is a fundamental preprocessing step in natural language processing (NLP).', 'It involves breaking down a text into smaller units called tokens.', 'Tokens are the building blocks of natural language and can be as short as a single character or as long as an entire word.', 'The process serves to structure and standardize textual data, facilitating tasks such as statistical analysis, language modeling, and information retrieval.Additionally, stemming is a related technique often employed in NLP preprocessing.', 'Stemming aims to reduce words to their base or root form, aiding in the consolidation of similar words with shared meanings.', 'This heuristic process involves removing prefixes or suffixes, resulting in stems that may not always be valid words but help in capturing the core meaning of related word forms.', 'Stemming contributes to text normalization and is commonly used to enhance the efficiency of text analysis and information retrieval systems.']\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "print(sentences2)\n",
    "print(len(sentences2))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T15:57:34.819283Z",
     "start_time": "2024-01-16T15:57:34.816375Z"
    }
   },
   "id": "efccc6e15dc9b2c1"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'a', 'fundamental', 'preprocessing', 'step', 'in', 'natural', 'language', 'processing', '(', 'NLP', ')', '.', 'It', 'involves', 'breaking', 'down', 'a', 'text', 'into', 'smaller', 'units', 'called', 'tokens', '.', 'Tokens', 'are', 'the', 'building', 'blocks', 'of', 'natural', 'language', 'and', 'can', 'be', 'as', 'short', 'as', 'a', 'single', 'character', 'or', 'as', 'long', 'as', 'an', 'entire', 'word', '.', 'The', 'process', 'serves', 'to', 'structure', 'and', 'standardize', 'textual', 'data', ',', 'facilitating', 'tasks', 'such', 'as', 'statistical', 'analysis', ',', 'language', 'modeling', ',', 'and', 'information', 'retrieval.Additionally', ',', 'stemming', 'is', 'a', 'related', 'technique', 'often', 'employed', 'in', 'NLP', 'preprocessing', '.', 'Stemming', 'aims', 'to', 'reduce', 'words', 'to', 'their', 'base', 'or', 'root', 'form', ',', 'aiding', 'in', 'the', 'consolidation', 'of', 'similar', 'words', 'with', 'shared', 'meanings', '.', 'This', 'heuristic', 'process', 'involves', 'removing', 'prefixes', 'or', 'suffixes', ',', 'resulting', 'in', 'stems', 'that', 'may', 'not', 'always', 'be', 'valid', 'words', 'but', 'help', 'in', 'capturing', 'the', 'core', 'meaning', 'of', 'related', 'word', 'forms', '.', 'Stemming', 'contributes', 'to', 'text', 'normalization', 'and', 'is', 'commonly', 'used', 'to', 'enhance', 'the', 'efficiency', 'of', 'text', 'analysis', 'and', 'information', 'retrieval', 'systems', '.']\n",
      "161\n"
     ]
    }
   ],
   "source": [
    "words2 = nltk.word_tokenize(text_stem)\n",
    "print(words2)\n",
    "print(len(words2))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T15:57:21.283010Z",
     "start_time": "2024-01-16T15:57:21.277526Z"
    }
   },
   "id": "5884b31ab86f08f3"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T15:58:49.553960Z",
     "start_time": "2024-01-16T15:58:49.546765Z"
    }
   },
   "id": "e7d6d5a0046dbf5c"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "['i',\n 'me',\n 'my',\n 'myself',\n 'we',\n 'our',\n 'ours',\n 'ourselves',\n 'you',\n \"you're\",\n \"you've\",\n \"you'll\",\n \"you'd\",\n 'your',\n 'yours',\n 'yourself',\n 'yourselves',\n 'he',\n 'him',\n 'his',\n 'himself',\n 'she',\n \"she's\",\n 'her',\n 'hers',\n 'herself',\n 'it',\n \"it's\",\n 'its',\n 'itself',\n 'they',\n 'them',\n 'their',\n 'theirs',\n 'themselves',\n 'what',\n 'which',\n 'who',\n 'whom',\n 'this',\n 'that',\n \"that'll\",\n 'these',\n 'those',\n 'am',\n 'is',\n 'are',\n 'was',\n 'were',\n 'be',\n 'been',\n 'being',\n 'have',\n 'has',\n 'had',\n 'having',\n 'do',\n 'does',\n 'did',\n 'doing',\n 'a',\n 'an',\n 'the',\n 'and',\n 'but',\n 'if',\n 'or',\n 'because',\n 'as',\n 'until',\n 'while',\n 'of',\n 'at',\n 'by',\n 'for',\n 'with',\n 'about',\n 'against',\n 'between',\n 'into',\n 'through',\n 'during',\n 'before',\n 'after',\n 'above',\n 'below',\n 'to',\n 'from',\n 'up',\n 'down',\n 'in',\n 'out',\n 'on',\n 'off',\n 'over',\n 'under',\n 'again',\n 'further',\n 'then',\n 'once',\n 'here',\n 'there',\n 'when',\n 'where',\n 'why',\n 'how',\n 'all',\n 'any',\n 'both',\n 'each',\n 'few',\n 'more',\n 'most',\n 'other',\n 'some',\n 'such',\n 'no',\n 'nor',\n 'not',\n 'only',\n 'own',\n 'same',\n 'so',\n 'than',\n 'too',\n 'very',\n 's',\n 't',\n 'can',\n 'will',\n 'just',\n 'don',\n \"don't\",\n 'should',\n \"should've\",\n 'now',\n 'd',\n 'll',\n 'm',\n 'o',\n 're',\n 've',\n 'y',\n 'ain',\n 'aren',\n \"aren't\",\n 'couldn',\n \"couldn't\",\n 'didn',\n \"didn't\",\n 'doesn',\n \"doesn't\",\n 'hadn',\n \"hadn't\",\n 'hasn',\n \"hasn't\",\n 'haven',\n \"haven't\",\n 'isn',\n \"isn't\",\n 'ma',\n 'mightn',\n \"mightn't\",\n 'mustn',\n \"mustn't\",\n 'needn',\n \"needn't\",\n 'shan',\n \"shan't\",\n 'shouldn',\n \"shouldn't\",\n 'wasn',\n \"wasn't\",\n 'weren',\n \"weren't\",\n 'won',\n \"won't\",\n 'wouldn',\n \"wouldn't\"]"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english') # there are a total of 179 stop words i.e., words which are generally not too important and are meant to be filtered out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T16:00:33.882043Z",
     "start_time": "2024-01-16T16:00:33.875642Z"
    }
   },
   "id": "107dc4b2e1aa7315"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "stem_words_final = []\n",
    "for i in range(len(sentences2)):\n",
    "    words = nltk.word_tokenize(sentences2[i])\n",
    "    stem_words = [stemmer.stem(items) for items in words if items not in set(stopwords.words('english'))]\n",
    "    stem_words_final.append(stem_words)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T17:52:02.524322Z",
     "start_time": "2024-01-16T17:52:02.502106Z"
    }
   },
   "id": "c452f377c51927d4"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "[['token',\n  'fundament',\n  'preprocess',\n  'step',\n  'natur',\n  'languag',\n  'process',\n  '(',\n  'nlp',\n  ')',\n  '.'],\n ['it', 'involv', 'break', 'text', 'smaller', 'unit', 'call', 'token', '.'],\n ['token',\n  'build',\n  'block',\n  'natur',\n  'languag',\n  'short',\n  'singl',\n  'charact',\n  'long',\n  'entir',\n  'word',\n  '.'],\n ['the',\n  'process',\n  'serv',\n  'structur',\n  'standard',\n  'textual',\n  'data',\n  ',',\n  'facilit',\n  'task',\n  'statist',\n  'analysi',\n  ',',\n  'languag',\n  'model',\n  ',',\n  'inform',\n  'retrieval.addit',\n  ',',\n  'stem',\n  'relat',\n  'techniqu',\n  'often',\n  'employ',\n  'nlp',\n  'preprocess',\n  '.'],\n ['stem',\n  'aim',\n  'reduc',\n  'word',\n  'base',\n  'root',\n  'form',\n  ',',\n  'aid',\n  'consolid',\n  'similar',\n  'word',\n  'share',\n  'mean',\n  '.'],\n ['thi',\n  'heurist',\n  'process',\n  'involv',\n  'remov',\n  'prefix',\n  'suffix',\n  ',',\n  'result',\n  'stem',\n  'may',\n  'alway',\n  'valid',\n  'word',\n  'help',\n  'captur',\n  'core',\n  'mean',\n  'relat',\n  'word',\n  'form',\n  '.'],\n ['stem',\n  'contribut',\n  'text',\n  'normal',\n  'commonli',\n  'use',\n  'enhanc',\n  'effici',\n  'text',\n  'analysi',\n  'inform',\n  'retriev',\n  'system',\n  '.']]"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_words_final"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T17:52:33.330121Z",
     "start_time": "2024-01-16T17:52:33.323834Z"
    }
   },
   "id": "bdb15f04b6a1d76"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ba77501e798a1352"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
