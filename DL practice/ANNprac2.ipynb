{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.12.0'"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width           class\n",
       "0             5.1          3.5           1.4          0.2     Iris-setosa\n",
       "1             4.9          3.0           1.4          0.2     Iris-setosa\n",
       "2             4.7          3.2           1.3          0.2     Iris-setosa\n",
       "3             4.6          3.1           1.5          0.2     Iris-setosa\n",
       "4             5.0          3.6           1.4          0.2     Iris-setosa\n",
       "..            ...          ...           ...          ...             ...\n",
       "145           6.7          3.0           5.2          2.3  Iris-virginica\n",
       "146           6.3          2.5           5.0          1.9  Iris-virginica\n",
       "147           6.5          3.0           5.2          2.0  Iris-virginica\n",
       "148           6.2          3.4           5.4          2.3  Iris-virginica\n",
       "149           5.9          3.0           5.1          1.8  Iris-virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "names = [\"sepal_length\",\"sepal_width\",\"petal_length\",\"petal_width\",\"class\"]\n",
    "df_flowers = pd.read_csv('iris.csv',sep=',',names =names) \n",
    "df_flowers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_flowers.iloc[:,:-1].values\n",
    "y = df_flowers.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica']\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot Encoding the independent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer   # the three countries France,Spain and Germany dont have any relation to each other hence it is one hot encoded\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "y = y.reshape(-1, 1)\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
    "y = np.array(ct.fit_transform(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "print(y.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.1 3.  4.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.  3.  4.8 1.8]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [4.9 3.1 1.5 0.1]]\n",
      "(120, 4)\n"
     ]
    }
   ],
   "source": [
    "print(X_train)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(y_train)\n",
    "print(y_train.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(y_test)\n",
    "print(y_test.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler  # note: For feature scaling in DL all the attributes need to be scaled unlike ML\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.31553662 -0.03612186  0.44748582  0.2345312 ]\n",
      " [ 2.2449325  -0.03612186  1.29803965  1.39642889]\n",
      " [-0.2873996  -1.240184    0.0505607  -0.15276803]\n",
      " [ 0.67729835 -0.51774672  1.01452171  1.13822941]\n",
      " [-0.04622511 -0.51774672  0.73100376  1.52552864]\n",
      " [-0.64916132  1.64956512 -1.31032543 -1.31466572]\n",
      " [-0.40798684 -1.72180885  0.10726429  0.10543146]\n",
      " [-0.76974857  0.92712784 -1.36702901 -1.31466572]\n",
      " [ 0.79788559 -0.51774672  0.44748582  0.36363094]\n",
      " [ 1.03906007 -1.240184    1.12792888  0.75093018]\n",
      " [ 1.15964732 -0.03612186  0.95781812  1.13822941]\n",
      " [-0.89033581  1.16794027 -1.36702901 -1.18556598]\n",
      " [ 0.19494938 -1.96262128  0.67430017  0.36363094]\n",
      " [ 0.5567111  -0.27693429  1.01452171  0.75093018]\n",
      " [ 0.91847283 -0.27693429  0.44748582  0.10543146]\n",
      " [ 2.2449325  -0.99937157  1.75166836  1.39642889]\n",
      " [-0.16681235  1.89037755 -1.19691825 -1.18556598]\n",
      " [-1.01092305  1.4087527  -1.36702901 -1.31466572]\n",
      " [-1.25209754 -0.03612186 -1.36702901 -1.18556598]\n",
      " [-0.76974857 -0.75855914  0.0505607   0.2345312 ]\n",
      " [-0.89033581  0.92712784 -1.31032543 -1.31466572]\n",
      " [-0.40798684  1.16794027 -1.4237326  -1.31466572]\n",
      " [ 2.2449325   1.89037755  1.63826118  1.26732915]\n",
      " [ 1.28023456  0.20469056  0.90111453  1.13822941]\n",
      " [ 1.76258353 -0.27693429  1.41144682  0.75093018]\n",
      " [ 0.67729835 -0.51774672  1.01452171  1.26732915]\n",
      " [ 0.43612386 -0.51774672  0.560893    0.75093018]\n",
      " [ 0.19494938  0.92712784  0.39078223  0.49273069]\n",
      " [ 2.2449325  -0.51774672  1.63826118  1.00912966]\n",
      " [-0.16681235 -0.03612186  0.22067147 -0.02366829]\n",
      " [-1.25209754  0.92712784 -1.25362184 -1.31466572]\n",
      " [-0.16681235 -1.240184    0.67430017  1.00912966]\n",
      " [ 0.5567111  -0.75855914  0.61759659  0.75093018]\n",
      " [-1.25209754 -0.03612186 -1.36702901 -1.44376547]\n",
      " [-1.37268478  0.44550299 -1.4237326  -1.31466572]\n",
      " [ 0.79788559 -0.03612186  1.12792888  1.26732915]\n",
      " [-1.49327202  0.92712784 -1.36702901 -1.18556598]\n",
      " [ 0.31553662 -0.03612186  0.61759659  0.75093018]\n",
      " [ 0.79788559  0.44550299  0.73100376  1.00912966]\n",
      " [ 1.03906007  0.20469056  0.33407864  0.2345312 ]\n",
      " [-0.16681235 -0.51774672  0.39078223  0.10543146]\n",
      " [ 1.03906007  0.68631542  1.07122529  1.65462838]\n",
      " [ 0.19494938 -0.03612186  0.560893    0.75093018]\n",
      " [-0.89033581  1.89037755 -1.25362184 -1.31466572]\n",
      " [ 0.19494938 -1.96262128  0.10726429 -0.28186777]\n",
      " [ 0.67729835 -0.27693429  0.27737505  0.10543146]\n",
      " [ 0.79788559 -0.03612186  0.95781812  0.75093018]\n",
      " [-1.01092305 -1.72180885 -0.28966083 -0.28186777]\n",
      " [ 0.5567111   0.68631542  1.24133606  1.65462838]\n",
      " [-0.40798684 -1.240184    0.10726429  0.10543146]\n",
      " [-0.52857408  1.64956512 -1.31032543 -1.31466572]\n",
      " [-1.13151029  0.20469056 -1.31032543 -1.44376547]\n",
      " [-0.76974857  2.61281483 -1.31032543 -1.44376547]\n",
      " [ 1.03906007  0.68631542  1.07122529  1.13822941]\n",
      " [-1.73444651 -0.03612186 -1.4237326  -1.31466572]\n",
      " [ 0.19494938 -0.75855914  0.73100376  0.49273069]\n",
      " [ 0.67729835 -0.75855914  0.84441094  0.88002992]\n",
      " [ 0.07436213 -0.03612186  0.73100376  0.75093018]\n",
      " [-0.76974857  1.16794027 -1.31032543 -1.31466572]\n",
      " [-0.89033581  0.68631542 -1.19691825 -0.92736649]\n",
      " [-0.04622511 -0.75855914  0.16396788 -0.28186777]\n",
      " [-1.13151029  0.20469056 -1.31032543 -1.44376547]\n",
      " [ 1.88317077 -0.51774672  1.29803965  0.88002992]\n",
      " [ 0.43612386 -0.27693429  0.27737505  0.10543146]\n",
      " [ 2.12434526 -0.03612186  1.58155759  1.13822941]\n",
      " [ 1.03906007 -0.03612186  0.78770735  1.39642889]\n",
      " [ 0.5567111  -1.72180885  0.33407864  0.10543146]\n",
      " [ 0.43612386  0.92712784  0.90111453  1.39642889]\n",
      " [ 1.64199629  1.4087527   1.29803965  1.65462838]\n",
      " [-0.2873996  -0.27693429 -0.11955007  0.10543146]\n",
      " [-0.16681235  3.33525211 -1.31032543 -1.05646624]\n",
      " [-0.04622511 -0.75855914  0.0505607  -0.02366829]\n",
      " [-1.61385927 -1.72180885 -1.4237326  -1.18556598]\n",
      " [-0.40798684 -1.48099642 -0.00614289 -0.15276803]\n",
      " [ 1.28023456  0.20469056  0.61759659  0.36363094]\n",
      " [-1.01092305  0.92712784 -1.25362184 -1.05646624]\n",
      " [ 1.15964732 -0.51774672  0.560893    0.2345312 ]\n",
      " [-1.01092305  1.16794027 -1.25362184 -0.79826675]\n",
      " [-1.25209754  0.92712784 -1.08351107 -1.31466572]\n",
      " [ 0.5567111   0.92712784  1.01452171  1.52552864]\n",
      " [-0.2873996  -0.51774672  0.61759659  1.00912966]\n",
      " [ 1.15964732  0.44550299  1.18463247  1.39642889]\n",
      " [-1.01092305  0.68631542 -1.36702901 -1.31466572]\n",
      " [-0.89033581  1.64956512 -1.31032543 -1.05646624]\n",
      " [ 0.07436213  0.44550299  0.560893    0.75093018]\n",
      " [-1.49327202  0.20469056 -1.31032543 -1.31466572]\n",
      " [-0.04622511 -0.75855914  0.73100376  0.88002992]\n",
      " [-1.25209754  0.20469056 -1.25362184 -1.31466572]\n",
      " [ 0.79788559 -0.03612186  0.78770735  1.00912966]\n",
      " [-1.13151029 -1.240184    0.39078223  0.62183043]\n",
      " [-1.49327202  0.44550299 -1.36702901 -1.31466572]\n",
      " [ 0.67729835  0.44550299  0.84441094  1.39642889]\n",
      " [-1.85503375 -0.03612186 -1.53713978 -1.44376547]\n",
      " [-0.2873996  -0.03612186  0.16396788  0.10543146]\n",
      " [-1.73444651 -0.27693429 -1.36702901 -1.31466572]\n",
      " [-0.40798684 -1.48099642 -0.06284648 -0.28186777]\n",
      " [-1.01092305 -2.44424613 -0.17625365 -0.28186777]\n",
      " [-0.89033581  1.16794027 -1.36702901 -1.31466572]\n",
      " [-1.13151029 -0.03612186 -1.36702901 -1.31466572]\n",
      " [-1.13151029 -1.48099642 -0.28966083 -0.28186777]\n",
      " [-1.49327202  1.4087527  -1.59384337 -1.31466572]\n",
      " [ 0.07436213 -0.03612186  0.22067147  0.36363094]\n",
      " [ 0.31553662 -0.27693429  0.50418941  0.2345312 ]\n",
      " [-1.01092305  0.92712784 -1.31032543 -1.31466572]\n",
      " [ 1.03906007  0.20469056  0.50418941  0.36363094]\n",
      " [-0.16681235 -0.27693429  0.22067147  0.10543146]\n",
      " [ 0.43612386 -1.96262128  0.39078223  0.36363094]\n",
      " [ 1.4008218   0.44550299  0.50418941  0.2345312 ]\n",
      " [-0.04622511 -0.75855914  0.73100376  0.88002992]\n",
      " [-0.52857408  0.92712784 -1.19691825 -1.31466572]\n",
      " [-1.01092305 -0.03612186 -1.25362184 -1.31466572]\n",
      " [ 0.31553662 -0.99937157  1.01452171  0.2345312 ]\n",
      " [ 0.31553662 -0.51774672  0.10726429  0.10543146]\n",
      " [ 1.64199629 -0.03612186  1.12792888  0.49273069]\n",
      " [-0.16681235 -0.99937157 -0.17625365 -0.28186777]\n",
      " [ 0.5567111  -0.51774672  0.73100376  0.36363094]\n",
      " [ 0.67729835  0.20469056  0.95781812  0.75093018]\n",
      " [ 0.5567111  -1.240184    0.61759659  0.36363094]\n",
      " [ 1.03906007  0.20469056  1.01452171  1.52552864]\n",
      " [-1.13151029  0.20469056 -1.31032543 -1.44376547]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.04622511  2.3720024  -1.48043619 -1.31466572]\n",
      " [-0.89033581 -1.240184   -0.4597716  -0.15276803]\n",
      " [ 0.91847283 -0.03612186  0.33407864  0.2345312 ]\n",
      " [-0.52857408  2.13118998 -1.4237326  -1.05646624]\n",
      " [ 2.48610699  1.89037755  1.46815041  1.00912966]\n",
      " [ 0.5567111   0.68631542  0.50418941  0.49273069]\n",
      " [ 1.28023456  0.20469056  0.73100376  1.39642889]\n",
      " [-0.89033581  1.89037755 -1.08351107 -1.05646624]\n",
      " [-1.37268478  0.44550299 -1.25362184 -1.31466572]\n",
      " [ 1.28023456  0.44550299  1.07122529  1.39642889]\n",
      " [-0.2873996  -0.75855914  0.22067147  0.10543146]\n",
      " [-0.52857408  2.13118998 -1.19691825 -1.05646624]\n",
      " [ 1.52140905 -0.03612186  1.18463247  1.13822941]\n",
      " [ 0.67729835  0.44550299  0.39078223  0.36363094]\n",
      " [ 0.19494938 -0.27693429  0.39078223  0.36363094]\n",
      " [-1.73444651  0.44550299 -1.4237326  -1.31466572]\n",
      " [-0.04622511 -0.99937157  0.10726429 -0.02366829]\n",
      " [-0.2873996  -0.03612186  0.39078223  0.36363094]\n",
      " [-0.52857408  0.92712784 -1.31032543 -1.05646624]\n",
      " [-1.01092305  0.44550299 -1.48043619 -1.31466572]\n",
      " [-0.40798684 -0.99937157  0.33407864 -0.02366829]\n",
      " [-0.52857408 -0.03612186  0.39078223  0.36363094]\n",
      " [ 1.03906007 -0.03612186  0.67430017  0.62183043]\n",
      " [-1.01092305  1.16794027 -1.4237326  -1.18556598]\n",
      " [ 1.64199629  0.44550299  1.24133606  0.75093018]\n",
      " [-0.16681235 -0.51774672  0.16396788  0.10543146]\n",
      " [-0.40798684  2.85362726 -1.36702901 -1.31466572]\n",
      " [-0.89033581  1.89037755 -1.31032543 -1.18556598]\n",
      " [ 0.31553662 -0.51774672  0.50418941 -0.02366829]\n",
      " [ 0.5567111  -1.240184    0.67430017  0.88002992]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Building the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the input layer and the first hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=6,activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the second hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=6,activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=3,activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Training the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the ANN on the Training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4/4 [==============================] - 0s 911us/step - loss: 1.1164 - accuracy: 0.2667\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 0s 679us/step - loss: 1.0908 - accuracy: 0.2583\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 0s 589us/step - loss: 1.0635 - accuracy: 0.2583\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 0s 586us/step - loss: 1.0388 - accuracy: 0.2917\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 0s 624us/step - loss: 1.0174 - accuracy: 0.3583\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 0s 593us/step - loss: 0.9957 - accuracy: 0.4167\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 0s 660us/step - loss: 0.9739 - accuracy: 0.4417\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 0s 603us/step - loss: 0.9555 - accuracy: 0.4500\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 0s 583us/step - loss: 0.9379 - accuracy: 0.4583\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 0s 702us/step - loss: 0.9208 - accuracy: 0.4583\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 0s 594us/step - loss: 0.9041 - accuracy: 0.4667\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 0s 593us/step - loss: 0.8904 - accuracy: 0.4750\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - 0s 641us/step - loss: 0.8757 - accuracy: 0.4750\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 0s 593us/step - loss: 0.8630 - accuracy: 0.4833\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 0s 553us/step - loss: 0.8503 - accuracy: 0.5083\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 0s 560us/step - loss: 0.8386 - accuracy: 0.5167\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 0s 662us/step - loss: 0.8278 - accuracy: 0.5167\n",
      "Epoch 18/100\n",
      "4/4 [==============================] - 0s 554us/step - loss: 0.8171 - accuracy: 0.5500\n",
      "Epoch 19/100\n",
      "4/4 [==============================] - 0s 529us/step - loss: 0.8070 - accuracy: 0.5500\n",
      "Epoch 20/100\n",
      "4/4 [==============================] - 0s 557us/step - loss: 0.7979 - accuracy: 0.5583\n",
      "Epoch 21/100\n",
      "4/4 [==============================] - 0s 602us/step - loss: 0.7884 - accuracy: 0.5750\n",
      "Epoch 22/100\n",
      "4/4 [==============================] - 0s 573us/step - loss: 0.7794 - accuracy: 0.6000\n",
      "Epoch 23/100\n",
      "4/4 [==============================] - 0s 661us/step - loss: 0.7709 - accuracy: 0.6167\n",
      "Epoch 24/100\n",
      "4/4 [==============================] - 0s 647us/step - loss: 0.7627 - accuracy: 0.6417\n",
      "Epoch 25/100\n",
      "4/4 [==============================] - 0s 621us/step - loss: 0.7548 - accuracy: 0.6333\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - 0s 530us/step - loss: 0.7470 - accuracy: 0.6333\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - 0s 532us/step - loss: 0.7393 - accuracy: 0.6417\n",
      "Epoch 28/100\n",
      "4/4 [==============================] - 0s 548us/step - loss: 0.7320 - accuracy: 0.6500\n",
      "Epoch 29/100\n",
      "4/4 [==============================] - 0s 513us/step - loss: 0.7246 - accuracy: 0.6667\n",
      "Epoch 30/100\n",
      "4/4 [==============================] - 0s 615us/step - loss: 0.7180 - accuracy: 0.6750\n",
      "Epoch 31/100\n",
      "4/4 [==============================] - 0s 582us/step - loss: 0.7111 - accuracy: 0.6750\n",
      "Epoch 32/100\n",
      "4/4 [==============================] - 0s 551us/step - loss: 0.7046 - accuracy: 0.6750\n",
      "Epoch 33/100\n",
      "4/4 [==============================] - 0s 551us/step - loss: 0.6981 - accuracy: 0.7083\n",
      "Epoch 34/100\n",
      "4/4 [==============================] - 0s 569us/step - loss: 0.6919 - accuracy: 0.7083\n",
      "Epoch 35/100\n",
      "4/4 [==============================] - 0s 564us/step - loss: 0.6860 - accuracy: 0.7167\n",
      "Epoch 36/100\n",
      "4/4 [==============================] - 0s 581us/step - loss: 0.6800 - accuracy: 0.7167\n",
      "Epoch 37/100\n",
      "4/4 [==============================] - 0s 539us/step - loss: 0.6744 - accuracy: 0.7250\n",
      "Epoch 38/100\n",
      "4/4 [==============================] - 0s 576us/step - loss: 0.6689 - accuracy: 0.7333\n",
      "Epoch 39/100\n",
      "4/4 [==============================] - 0s 532us/step - loss: 0.6636 - accuracy: 0.7333\n",
      "Epoch 40/100\n",
      "4/4 [==============================] - 0s 530us/step - loss: 0.6583 - accuracy: 0.7333\n",
      "Epoch 41/100\n",
      "4/4 [==============================] - 0s 681us/step - loss: 0.6530 - accuracy: 0.7333\n",
      "Epoch 42/100\n",
      "4/4 [==============================] - 0s 530us/step - loss: 0.6478 - accuracy: 0.7333\n",
      "Epoch 43/100\n",
      "4/4 [==============================] - 0s 579us/step - loss: 0.6427 - accuracy: 0.7333\n",
      "Epoch 44/100\n",
      "4/4 [==============================] - 0s 548us/step - loss: 0.6378 - accuracy: 0.7333\n",
      "Epoch 45/100\n",
      "4/4 [==============================] - 0s 526us/step - loss: 0.6327 - accuracy: 0.7333\n",
      "Epoch 46/100\n",
      "4/4 [==============================] - 0s 561us/step - loss: 0.6277 - accuracy: 0.7417\n",
      "Epoch 47/100\n",
      "4/4 [==============================] - 0s 500us/step - loss: 0.6227 - accuracy: 0.7417\n",
      "Epoch 48/100\n",
      "4/4 [==============================] - 0s 637us/step - loss: 0.6180 - accuracy: 0.7417\n",
      "Epoch 49/100\n",
      "4/4 [==============================] - 0s 561us/step - loss: 0.6132 - accuracy: 0.7500\n",
      "Epoch 50/100\n",
      "4/4 [==============================] - 0s 525us/step - loss: 0.6085 - accuracy: 0.7500\n",
      "Epoch 51/100\n",
      "4/4 [==============================] - 0s 501us/step - loss: 0.6039 - accuracy: 0.7583\n",
      "Epoch 52/100\n",
      "4/4 [==============================] - 0s 585us/step - loss: 0.5992 - accuracy: 0.7583\n",
      "Epoch 53/100\n",
      "4/4 [==============================] - 0s 587us/step - loss: 0.5948 - accuracy: 0.7583\n",
      "Epoch 54/100\n",
      "4/4 [==============================] - 0s 587us/step - loss: 0.5904 - accuracy: 0.7583\n",
      "Epoch 55/100\n",
      "4/4 [==============================] - 0s 585us/step - loss: 0.5857 - accuracy: 0.7583\n",
      "Epoch 56/100\n",
      "4/4 [==============================] - 0s 554us/step - loss: 0.5813 - accuracy: 0.7583\n",
      "Epoch 57/100\n",
      "4/4 [==============================] - 0s 543us/step - loss: 0.5769 - accuracy: 0.7583\n",
      "Epoch 58/100\n",
      "4/4 [==============================] - 0s 645us/step - loss: 0.5727 - accuracy: 0.7583\n",
      "Epoch 59/100\n",
      "4/4 [==============================] - 0s 519us/step - loss: 0.5684 - accuracy: 0.7583\n",
      "Epoch 60/100\n",
      "4/4 [==============================] - 0s 577us/step - loss: 0.5644 - accuracy: 0.7583\n",
      "Epoch 61/100\n",
      "4/4 [==============================] - 0s 593us/step - loss: 0.5603 - accuracy: 0.7583\n",
      "Epoch 62/100\n",
      "4/4 [==============================] - 0s 482us/step - loss: 0.5564 - accuracy: 0.7583\n",
      "Epoch 63/100\n",
      "4/4 [==============================] - 0s 501us/step - loss: 0.5525 - accuracy: 0.7583\n",
      "Epoch 64/100\n",
      "4/4 [==============================] - 0s 619us/step - loss: 0.5485 - accuracy: 0.7667\n",
      "Epoch 65/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.5446 - accuracy: 0.7667\n",
      "Epoch 66/100\n",
      "4/4 [==============================] - 0s 709us/step - loss: 0.5407 - accuracy: 0.7667\n",
      "Epoch 67/100\n",
      "4/4 [==============================] - 0s 652us/step - loss: 0.5367 - accuracy: 0.7667\n",
      "Epoch 68/100\n",
      "4/4 [==============================] - 0s 733us/step - loss: 0.5329 - accuracy: 0.7750\n",
      "Epoch 69/100\n",
      "4/4 [==============================] - 0s 628us/step - loss: 0.5288 - accuracy: 0.7750\n",
      "Epoch 70/100\n",
      "4/4 [==============================] - 0s 679us/step - loss: 0.5245 - accuracy: 0.7750\n",
      "Epoch 71/100\n",
      "4/4 [==============================] - 0s 675us/step - loss: 0.5204 - accuracy: 0.7750\n",
      "Epoch 72/100\n",
      "4/4 [==============================] - 0s 553us/step - loss: 0.5167 - accuracy: 0.7833\n",
      "Epoch 73/100\n",
      "4/4 [==============================] - 0s 627us/step - loss: 0.5128 - accuracy: 0.7833\n",
      "Epoch 74/100\n",
      "4/4 [==============================] - 0s 597us/step - loss: 0.5091 - accuracy: 0.7917\n",
      "Epoch 75/100\n",
      "4/4 [==============================] - 0s 567us/step - loss: 0.5051 - accuracy: 0.7917\n",
      "Epoch 76/100\n",
      "4/4 [==============================] - 0s 602us/step - loss: 0.5017 - accuracy: 0.7917\n",
      "Epoch 77/100\n",
      "4/4 [==============================] - 0s 600us/step - loss: 0.4982 - accuracy: 0.8000\n",
      "Epoch 78/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4945 - accuracy: 0.8000\n",
      "Epoch 79/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4911 - accuracy: 0.8000\n",
      "Epoch 80/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4875 - accuracy: 0.8083\n",
      "Epoch 81/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.4842 - accuracy: 0.8083\n",
      "Epoch 82/100\n",
      "4/4 [==============================] - 0s 774us/step - loss: 0.4810 - accuracy: 0.8083\n",
      "Epoch 83/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.4778 - accuracy: 0.8083\n",
      "Epoch 84/100\n",
      "4/4 [==============================] - 0s 947us/step - loss: 0.4744 - accuracy: 0.8083\n",
      "Epoch 85/100\n",
      "4/4 [==============================] - 0s 725us/step - loss: 0.4714 - accuracy: 0.8083\n",
      "Epoch 86/100\n",
      "4/4 [==============================] - 0s 629us/step - loss: 0.4682 - accuracy: 0.8083\n",
      "Epoch 87/100\n",
      "4/4 [==============================] - 0s 657us/step - loss: 0.4650 - accuracy: 0.8083\n",
      "Epoch 88/100\n",
      "4/4 [==============================] - 0s 671us/step - loss: 0.4620 - accuracy: 0.8083\n",
      "Epoch 89/100\n",
      "4/4 [==============================] - 0s 574us/step - loss: 0.4591 - accuracy: 0.8083\n",
      "Epoch 90/100\n",
      "4/4 [==============================] - 0s 579us/step - loss: 0.4563 - accuracy: 0.8167\n",
      "Epoch 91/100\n",
      "4/4 [==============================] - 0s 592us/step - loss: 0.4533 - accuracy: 0.8167\n",
      "Epoch 92/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.4505 - accuracy: 0.8167\n",
      "Epoch 93/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.4477 - accuracy: 0.8167\n",
      "Epoch 94/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4447 - accuracy: 0.8167\n",
      "Epoch 95/100\n",
      "4/4 [==============================] - 0s 693us/step - loss: 0.4421 - accuracy: 0.8167\n",
      "Epoch 96/100\n",
      "4/4 [==============================] - 0s 531us/step - loss: 0.4394 - accuracy: 0.8167\n",
      "Epoch 97/100\n",
      "4/4 [==============================] - 0s 564us/step - loss: 0.4366 - accuracy: 0.8167\n",
      "Epoch 98/100\n",
      "4/4 [==============================] - 0s 555us/step - loss: 0.4339 - accuracy: 0.8167\n",
      "Epoch 99/100\n",
      "4/4 [==============================] - 0s 585us/step - loss: 0.4314 - accuracy: 0.8167\n",
      "Epoch 100/100\n",
      "4/4 [==============================] - 0s 574us/step - loss: 0.4290 - accuracy: 0.8167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16847be90>"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.fit(X_train, y_train,batch_size = 32,epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step\n",
      "[[9.8470140e-01 1.5115026e-02 1.8358523e-04]\n",
      " [2.6621997e-01 6.8729907e-01 4.6480991e-02]\n",
      " [4.7579505e-02 3.2079867e-01 6.3162190e-01]\n",
      " [9.8387456e-01 1.5984990e-02 1.4037683e-04]\n",
      " [4.8818797e-02 3.2114136e-01 6.3003987e-01]\n",
      " [7.9999447e-02 3.2452396e-01 5.9547657e-01]\n",
      " [2.9773409e-02 3.1298271e-01 6.5724385e-01]\n",
      " [9.7983861e-01 1.9971713e-02 1.8965469e-04]\n",
      " [9.8589176e-01 1.4084466e-02 2.3719351e-05]\n",
      " [3.5200823e-02 3.1607261e-01 6.4872658e-01]\n",
      " [1.0180827e-01 5.3312504e-01 3.6506671e-01]\n",
      " [9.7885644e-01 2.0870438e-02 2.7306666e-04]\n",
      " [2.5826361e-02 3.1015792e-01 6.6401571e-01]\n",
      " [6.8282679e-02 3.2447451e-01 6.0724282e-01]\n",
      " [5.0021023e-02 3.2325691e-01 6.2672216e-01]\n",
      " [9.9189359e-01 8.1006503e-03 5.6810654e-06]\n",
      " [9.4924025e-02 5.4001814e-01 3.6505780e-01]\n",
      " [6.5684438e-02 3.4306568e-01 5.9124988e-01]\n",
      " [9.6342778e-01 3.6191821e-02 3.8034417e-04]\n",
      " [9.8105961e-01 1.8895717e-02 4.4663819e-05]\n",
      " [1.0245700e-01 5.7994080e-01 3.1760219e-01]\n",
      " [1.0587598e-01 4.2993781e-01 4.6418622e-01]\n",
      " [3.9574042e-02 3.1805187e-01 6.4237404e-01]\n",
      " [9.8493963e-01 1.5006680e-02 5.3668231e-05]\n",
      " [3.7080519e-02 3.1697208e-01 6.4594746e-01]\n",
      " [1.0016940e-01 4.8233205e-01 4.1749853e-01]\n",
      " [9.9007809e-01 9.8335603e-03 8.8258370e-05]\n",
      " [9.8623639e-01 1.3684115e-02 7.9434343e-05]\n",
      " [4.8659064e-02 3.2132745e-01 6.3001347e-01]\n",
      " [1.8131776e-02 2.9980704e-01 6.8206114e-01]]\n",
      "iris setosa\n",
      "iris versicolor\n",
      "iris virginica\n",
      "iris setosa\n",
      "iris virginica\n",
      "iris virginica\n",
      "iris virginica\n",
      "iris setosa\n",
      "iris setosa\n",
      "iris virginica\n",
      "iris versicolor\n",
      "iris setosa\n",
      "iris virginica\n",
      "iris virginica\n",
      "iris virginica\n",
      "iris setosa\n",
      "iris versicolor\n",
      "iris virginica\n",
      "iris setosa\n",
      "iris setosa\n",
      "iris versicolor\n",
      "iris virginica\n",
      "iris virginica\n",
      "iris setosa\n",
      "iris virginica\n",
      "iris versicolor\n",
      "iris setosa\n",
      "iris setosa\n",
      "iris virginica\n",
      "iris virginica\n"
     ]
    }
   ],
   "source": [
    "y_pred = ann.predict(X_test)\n",
    "print(y_pred)\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i][0] == max(y_pred[i]):\n",
    "        print(\"iris setosa\")\n",
    "    elif(y_pred[i][1] == max(y_pred[i])):\n",
    "        print(\"iris versicolor\")\n",
    "    else:\n",
    "        print(\"iris virginica\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "iris setosa\n",
      "iris versicolor\n",
      "iris virginica\n",
      "iris setosa\n",
      "iris virginica\n",
      "iris virginica\n",
      "iris virginica\n",
      "iris setosa\n",
      "iris setosa\n",
      "iris virginica\n",
      "iris versicolor\n",
      "iris setosa\n",
      "iris virginica\n",
      "iris virginica\n",
      "iris virginica\n",
      "iris setosa\n",
      "iris versicolor\n",
      "iris virginica\n",
      "iris setosa\n",
      "iris setosa\n",
      "iris versicolor\n",
      "iris virginica\n",
      "iris virginica\n",
      "iris setosa\n",
      "iris virginica\n",
      "iris versicolor\n",
      "iris setosa\n",
      "iris setosa\n",
      "iris virginica\n",
      "iris virginica\n"
     ]
    }
   ],
   "source": [
    "print(y_test)\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i][0] == max(y_pred[i]):\n",
    "        print(\"iris setosa\")\n",
    "    elif(y_pred[i][1] == max(y_pred[i])):\n",
    "        print(\"iris versicolor\")\n",
    "    else:\n",
    "        print(\"iris virginica\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(y_test.ndim )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted Class\")\n",
    "plt.ylabel(\"True Class\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "arr = [[1, 2, 3],\n",
    " [4, 5, 6],\n",
    " [7, 8, 9]]\n",
    "print(arr[1][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
